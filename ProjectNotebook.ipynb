{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fe7b45",
   "metadata": {},
   "source": [
    "# Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe8865",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3960b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 17:31:32.655636: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-26 17:31:32.655659: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, Add, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.nn import depth_to_space\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ea2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import downsample, normalize\n",
    "from SRCNN import srcnn\n",
    "import DIPCI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a52f",
   "metadata": {},
   "source": [
    "### Path & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3bd5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"./Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8be37",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67372f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ssh = PATH_DATA+\"/1308_square_NATL60_SSH_R09.npy\"\n",
    "data_sst = PATH_DATA+\"/1308_square_NATL60_SST_R09.npy\"\n",
    "ssh = np.load(data_ssh)\n",
    "sst = np.load(data_sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421dc2fc",
   "metadata": {},
   "source": [
    "HR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7690a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_hr_train = ssh[0:366, :, :]\n",
    "ssh_hr_test  = ssh[366:, :, :]\n",
    "sst_hr_train = sst[0:366, :, :]\n",
    "sst_hr_test  = sst[366:, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db407d7",
   "metadata": {},
   "source": [
    "LR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d703c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampling_factor = 2\n",
    "ssh_lr_train = np.array([downsample(img, downsampling_factor) for img in ssh_hr_train])\n",
    "ssh_lr_test  = np.array([downsample(img, downsampling_factor) for img in ssh_hr_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a31c19",
   "metadata": {},
   "source": [
    "Normalize (SSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58556bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_hr_train = np.array([normalize(img) for img in ssh_hr_train])\n",
    "ssh_lr_train = np.array([normalize(img) for img in ssh_lr_train])\n",
    "ssh_hr_test  = np.array([normalize(img) for img in ssh_hr_test])\n",
    "ssh_lr_test  = np.array([normalize(img) for img in ssh_lr_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b532d",
   "metadata": {},
   "source": [
    "### SRCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769cb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.003)\n",
    "checkpoint = ModelCheckpoint(\"SRCNN_check.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                 save_weights_only=False, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2564b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 17:31:35.201006: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-26 17:31:35.201040: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-26 17:31:35.201061: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3800411-Latitude-E7270): /proc/driver/nvidia/version does not exist\n",
      "2022-04-26 17:31:35.201300: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "SRCNN = srcnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00d92fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 861, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 818, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=<keras.losses.MeanSquaredError object at 0x7fefb2dd1400>, and therefore expects target data to be provided in `fit()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10333/1627293566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSRCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSRCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mssh_lr_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mssh_hr_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 861, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/home/matt/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 818, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=<keras.losses.MeanSquaredError object at 0x7fefb2dd1400>, and therefore expects target data to be provided in `fit()`.\n"
     ]
    }
   ],
   "source": [
    "SRCNN.compile(optimizer=optimizer, loss=loss_fn)\n",
    "SRCNN.fit( ssh_lr_train, epochs=epochs, callbacks=callbacks, validation_data=ssh_hr_train.all(), verbose=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8da6c9",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 64\n",
    "upscale_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_conv_model( shape=1, filters=filters ):\n",
    "    ''' extracts SST features '''\n",
    "    input_img = Input(shape=(None, None, shape))\n",
    "    x = Conv2D(filters, (3,3), padding='same')(input_img)\n",
    "    x = LeakyReLU(alpha=0.2)(x)        \n",
    "    x = Conv2D(filters, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return Model(inputs=input_img, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_model( filters=filters ):\n",
    "    ''' upsamples in feature branch '''\n",
    "    input_img = Input(shape=(None, None, filters))\n",
    "    x = Conv2D(filters, (3,3), padding='same')(input_img)\n",
    "    x = depth_to_space(x, upscale_factor)        \n",
    "    x = Conv2D(filters, (2,2), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    return Model(inputs=input_img, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59160839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_model( shape=1, filters=filters ):\n",
    "    ''' upsamples SSH '''\n",
    "    input_img = Input(shape=(None, None, shape))\n",
    "    upsample = Conv2DTranspose(filters, (4,4), strides=(2,2), padding='same')(input_img)\n",
    "    return Model(inputs=input_img, outputs=upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net():\n",
    "    simple_conv_1 = simple_conv_model(1)\n",
    "    simple_conv_2 = simple_conv_model(filters)\n",
    "    shuffle = shuffle_model()\n",
    "    #upsample_1 = upsample_1_model()\n",
    "    upsample_1 = upsample_model(1)\n",
    "    upsample_2 = upsample_model(filters)\n",
    "    \n",
    "    input_hr = Input(shape=(None, None, 1))\n",
    "    input_lr = Input(shape=(None, None, 1))\n",
    "    \n",
    "    # firstPass\n",
    "    x = simple_conv_1(input_hr)\n",
    "    y = upsample_1(input_lr)\n",
    "    hr = Add()([y, x])\n",
    "    # secondPass\n",
    "    x = shuffle(x)\n",
    "    x = simple_conv_2(x)\n",
    "    y = upsample_2(hr)\n",
    "    hr = Add()([y, x])\n",
    "    \n",
    "    return Model(inputs=[input_lr, input_hr], outputs=hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24be92d",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66788da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net()\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836d85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
